{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recolección data CoAID(Covid-19 heAlthcare mIsinformation Dataset)\n",
    "La desinformación relacionada con COVID-19 también se crea y se propaga como la pólvora. Esta desinformación ha causado confusión entre las personas, trastornos en la sociedad e incluso consecuencias mortales en problemas de salud.\n",
    "\n",
    "Este notebook nos permite obtener la data almacenada en el siguiente repo: https://github.com/cuilimeng/CoAID y almacenarla en una base de datos local.\n",
    "\n",
    "Asimismo, solo los tweets son hidratados mediante la API de Twitter, para cual se desarrolló un algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import get_connection_covid_coaid,get_df_sql\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection_covid_coaid()\n",
    "BASE_URL_DATASET = \"https://raw.githubusercontent.com/cuilimeng/CoAID/master/\"\n",
    "DATA_DIR= '/home/reisson/Documents/platzi_interview/owid/CoAID/data'\n",
    "os.mkdir(DATA_DIR)\n",
    "PATH_FAKE_TWEETS = DATA_DIR+'/fake-tweets'\n",
    "PATH_REAL_TWEETS = DATA_DIR+'/real-tweets'\n",
    "CONSUMER_KEY='GV8cYa5GSjk27F2JZb6z8MrsJ'\n",
    "CONSUMER_SECRET_KEY='mybd68vqhP6wSyvQOtxlYoc9aRne08FAImpchhL679W2vgNfsS'\n",
    "ACCESS_TOKEN_KEY='1085983643640709121-qEMfxY6RSmPHHaEvDxIIsEXSjn2y5v'\n",
    "ACCESS_TOKEN_SECRET_KEY='jZ02dDaFlPNC5gJI5PTuV9BeivolkPlPxcLK8LuAPDEME'\n",
    "DATE_ARR = ['05-01-2020','07-01-2020','09-01-2020','11-01-2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET_KEY)\n",
    "auth.set_access_token(ACCESS_TOKEN_KEY,ACCESS_TOKEN_SECRET_KEY)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, \n",
    "                 retry_delay=60*3, \n",
    "                 retry_count=5,\n",
    "                 retry_errors=set([401, 404, 500, 503]), \n",
    "                 wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweet(tweet):\n",
    "    try:        \n",
    "        indices = [i for i, x in enumerate(tweet) if x == \"'\"]\n",
    "        if indices == []:\n",
    "            return tweet\n",
    "        var = 0\n",
    "        for a in indices:\n",
    "            tweet = tweet[:a+var] + \"'\" + tweet[a+var:]\n",
    "            var = var + 1\n",
    "        return tweet\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sp_ins_class(tweet_id,tweet_class,line_count):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        call_sp = f\"CALL public.ins_class_tweet('{tweet_id}','{tweet_class}');\"\n",
    "        cur.execute(call_sp)\n",
    "        conn.commit()\n",
    "        line_count=line_count+1\n",
    "        print(f'Processed {line_count} lines.')\n",
    "    except(Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        cur.close()\n",
    "    return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retweet(entry):\n",
    "    return 'retweeted_status' in entry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(entry):\n",
    "    if '<' in entry[\"source\"]:\n",
    "        return entry[\"source\"].split('>')[1].split('<')[0]\n",
    "    else:\n",
    "        return entry[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sp_ins_class(tweet_id,tweet_class,line_count):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        call_sp = f\"CALL public.ins_class_tweet('{tweet_id}','{tweet_class}');\"\n",
    "        cur.execute(call_sp)\n",
    "        conn.commit()\n",
    "        line_count=line_count+1\n",
    "        print(f'Processed {line_count} lines.')\n",
    "    except(Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        cur.close()\n",
    "    return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ins_class_tweet_from_csv(path_csv):\n",
    "    line_count = 0\n",
    "    class_tweet = path_csv.split('/')[-1].split('-')[0]\n",
    "    with open(path_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if row[1] != 'tweet_id':\n",
    "                line_count = call_sp_ins_class(row[1],class_tweet,line_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sp_ins_hydrated_tweet(data):\n",
    "    CONN = psycopg2.connect(host=HOST,\n",
    "                            database=DATABASE,\n",
    "                            user=USER,\n",
    "                            password=PASSWORD)\n",
    "    call_sp=''\n",
    "    \n",
    "    tweet_prepared = prepare_tweet(data[\"text\"])\n",
    "    source_data = prepare_tweet(get_source(data))\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        call_sp += \"CALL public.insert_hydrated_tweet('\"\n",
    "        call_sp += str(data[\"id_str\"]) + \"',quote_literal('\"\n",
    "        call_sp += tweet_prepared + \"'),NULL,'\"\n",
    "        call_sp += data[\"created_at\"] + \"',\"\n",
    "        call_sp += str(is_retweet(data)) + \",'\"\n",
    "        call_sp += source_data + \"',\"\n",
    "        call_sp += str(data[\"favorite_count\"]) + \",\"\n",
    "        call_sp += str(data[\"retweet_count\"]) + \")\"\n",
    "        cur.execute(call_sp)\n",
    "        conn.commit()\n",
    "    except(Exception,psycopg2.DatabaseError) as error:\n",
    "        print(f'Se encontro el siguiente error: {error}')\n",
    "    finally:\n",
    "        cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in DATE_ARR:\n",
    "    print('read database '+ i)\n",
    "    url_fake_news = BASE_URL_DATASET+i+\"/\"+\"NewsFakeCOVID-19_tweets.csv?raw=true\"\n",
    "    path_fake_news = PATH_FAKE_TWEETS+'-'+i+'.csv'\n",
    "    url_real_news = BASE_URL_DATASET+i+\"/\"+\"NewsRealCOVID-19_tweets.csv?raw=true\"\n",
    "    path_real_news = PATH_REAL_TWEETS+'-'+i+'.csv'\n",
    "    wget.download(url_fake_news, out=path_fake_news)\n",
    "    wget.download(url_real_news, out=path_real_news)\n",
    "    ins_class_tweet_from_csv(path_fake_news)\n",
    "    ins_class_tweet_from_csv(path_real_news)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 100\n",
    "limit = len(ids)\n",
    "print(f'Len ids:{len(ids)}')\n",
    "i = int(math.ceil(float(limit) / 100))\n",
    "\n",
    "output_file = DATA_DIR + '/json_data'\n",
    "\n",
    "last_tweet = None\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET_KEY)\n",
    "auth.set_access_token(ACCESS_TOKEN_KEY,ACCESS_TOKEN_SECRET_KEY)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, \n",
    "                 retry_delay=60*3, \n",
    "                 retry_count=5,\n",
    "                 retry_errors=set([401, 404, 500, 503]), \n",
    "                 wait_on_rate_limit_notify=True)\n",
    "\n",
    "    \n",
    "if api.verify_credentials() == False: \n",
    "    print('Credenciales no validas') \n",
    "    sys.exit()\n",
    "else: \n",
    "    print(\"Credenciales validas\") \n",
    "\n",
    "if osp.isfile(output_file) and osp.getsize(output_file) > 0:\n",
    "    with open(output_file, 'rb') as f:\n",
    "            #may be a large file, seeking without iterating\n",
    "        f.seek(-2, os.SEEK_END)\n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, os.SEEK_CUR)\n",
    "        last_line = f.readline().decode()\n",
    "    last_tweet = json.loads(last_line)\n",
    "    print(ids.index((last_tweet['id']))) \n",
    "    start = ids.index(last_tweet['id'])\n",
    "    end = start+100\n",
    "    i = int(math.ceil(float(limit-start) / 100))\n",
    "\n",
    "print('Coleccion de metadata')\n",
    "print('Creando master json file')\n",
    "\n",
    "try:\n",
    "    with open(output_file, 'a') as outfile:\n",
    "        for go in range(i):\n",
    "            print('Obteniendo el rango {} - {}'.format(start, end))\n",
    "            sleep(6)  # needed to prevent hitting API rate limit\n",
    "            id_batch = ids[start:end]\n",
    "            start += 100\n",
    "            end += 100       \n",
    "            backOffCounter = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    tweets = api.statuses_lookup(id_batch)\n",
    "                    break\n",
    "                except tweepy.TweepError as ex:\n",
    "                    print('Se capturo TweepError exception:\\n %s' % ex)\n",
    "                    sleep(30*backOffCounter)  # sleep a bit to see if connection Error is resolved before retrying\n",
    "                    backOffCounter += 1  # increase backoff\n",
    "                    continue\n",
    "\n",
    "            for tweet in tweets:\n",
    "                call_sp_ins_hydrated_tweet(tweet._json)\n",
    "                json.dump(tweet._json, outfile)\n",
    "                outfile.write('\\n')\n",
    "except:\n",
    "    print('exception: continuing to zip the file' + ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
